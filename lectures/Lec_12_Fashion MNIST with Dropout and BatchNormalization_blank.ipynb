{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec_10_Fashion MNIST\n",
    "<font size=5><b>Dropout and BatchNormalization<b></font>\n",
    "<div align='right'> Hoe Sung Ryu ( 류 회 성 ) </div>\n",
    "    \n",
    "\n",
    "\n",
    "<img src= https://leejunhyun.github.io/assets/img/TensorFlow/Fashion-MNIST-01.png>\n",
    "    \n",
    "---\n",
    "    \n",
    "Syllabus\n",
    "\n",
    "|Event Type|Date|Topic|\n",
    "|--:|:---:|:---|\n",
    "|1 |July 27| Environment setting and Python basic|\n",
    "|2 |July 28| Pytorch basic and Custom Data load |\n",
    "|3 |July 29| Traditional Machine Learning(1) |\n",
    "|4 |July 30| Traditional Machine Learning(2) |\n",
    "|5 |July 31| CNN(Convolutional Neural Network)(1)  |\n",
    "|6 |Aug 03| CNN(Convolutional NeuralNetwork)(2) |\n",
    "|7 |Aug 04|  RNN(Recurrent Neural Networks)(1) |\n",
    "|8 |Aug 05|  RNN(Recurrent Neural Networks)(2) |\n",
    "|9 |Aug 06|  Transfer learning(VGG pertained on ImageNEt for CIfar-10)| \n",
    "|10|Aug 07|**Mini_Kaggle**: Facial Expression Recognition on `AffectNet` | \n",
    "|11|Aug 08|`Awards` and `Closing`| \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Review\" data-toc-modified-id=\"Review-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Review</a></span></li><li><span><a href=\"#Today\" data-toc-modified-id=\"Today-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Today</a></span></li><li><span><a href=\"#BatchNormalization\" data-toc-modified-id=\"BatchNormalization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>BatchNormalization</a></span></li><li><span><a href=\"#Drop-out\" data-toc-modified-id=\"Drop-out-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Drop out</a></span></li><li><span><a href=\"#Ordering-of-batch-normalization,-dropout,-Activation\" data-toc-modified-id=\"Ordering-of-batch-normalization,-dropout,-Activation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Ordering of batch normalization, dropout, Activation</a></span></li><li><span><a href=\"#Dataset-and-DataLoader\" data-toc-modified-id=\"Dataset-and-DataLoader-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Dataset and DataLoader</a></span></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>EDA</a></span></li><li><span><a href=\"#Model-Build\" data-toc-modified-id=\"Model-Build-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model Build</a></span></li><li><span><a href=\"#Set-Loss-and-Optimizer\" data-toc-modified-id=\"Set-Loss-and-Optimizer-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Set Loss and Optimizer</a></span></li><li><span><a href=\"#Train-and-Test\" data-toc-modified-id=\"Train-and-Test-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Train and Test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Review\n",
    "\n",
    "<img src='../img/playground.png'>\n",
    "<center>Visual illustration of connection between neural network architecture, hyperparameters, and dataset characteristics.</center> \n",
    "<br>\n",
    "    \n",
    "Explore this connection yourself at: https://playground.tensorflow.org/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "- Change Batch Size\n",
    "- Dropout Layer \n",
    "- BatchNorm Layer\n",
    "- Linear-->activation-->BatchNorm-->Dropout Order for MLP\n",
    "\n",
    "reference:\n",
    "- https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BatchNormalization\n",
    "https://kharshit.github.io/blog/2018/12/28/why-batch-normalization\n",
    "\n",
    "\n",
    "In deep neural networks, you not only have input features but activations in the hidden layers also. Can/Should you normalize them also? The answer is Yes. Normalizing the inputs to hidden layers helps in faster learning. This the core concept of batch normalization\n",
    "<img src=https://kharshit.github.io/img/batch_normalization.png>\n",
    "\n",
    " during training, we normalize each layer’s inputs by using the mean and standard deviation (or variance) of the values in the current batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out\n",
    "\n",
    "\"[Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014.](http://jmlr.org/papers/v15/srivastava14a.html)\"\n",
    "\n",
    "\n",
    "Large neural networks trained on relatively small datasets can overfit the training data.One approach to reduce overfitting is to fit all possible different neural networks on the same dataset and to average the predictions from each model called `ensemble`. A problem with the `ensemble` approximation is that it requires multiple models to be fit and stored, which can be a challenge if the models are large, requiring days to train.\n",
    "\n",
    "However, `Dropout` is a regularization method that approximates training a large number of neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored. This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer.\n",
    "\n",
    "\n",
    "<img src=https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering of batch normalization, dropout, Activation\n",
    "\n",
    "\n",
    "The original `BatchNorm paper` prescribes using `BN` before` ReLU`. The following is the exact text from the paper: <p>\n",
    "> We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b. We could have also normalized the layer inputs u, but since u is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the covariate shift. In contrast, Wu + b is more likely to have a symmetric, non-sparse distribution, that is “more Gaussian” (Hyv¨arinen & Oja, 2000); normalizing it is likely to produce activations with a stable distribution.\n",
    "\n",
    "However, in practice I find that the opposite is true - BN after ReLU consistently performs better. \n",
    "_From [Reddit](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)_\n",
    "\n",
    "Hence, I recommend `Linear --> Activation --> BatchNorm --> Dropout`\n",
    "    \n",
    "    \n",
    "reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout#:~:text=For%20example%2C%20if%20the%20shift,that%20shift%20may%20be%20off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:19.392442Z",
     "start_time": "2020-07-29T16:23:18.480756Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/',\n",
    "                                 download=False,\n",
    "                                 train=True, \n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.5),\n",
    "                                                                                    (0.5))]))\n",
    "\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/',\n",
    "                                 download=False,\n",
    "                                 train=False, \n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.5),\n",
    "                                                                                    (0.5))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommand: change batch_size 2^n\n",
    "# TODO \n",
    "train_loader = \n",
    "test_loader = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:20.099236Z",
     "start_time": "2020-07-29T16:23:19.799052Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:20.845276Z",
     "start_time": "2020-07-29T16:23:20.176418Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n",
    "              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'};\n",
    "fig = plt.figure(figsize=(8,8));\n",
    "columns = 4;\n",
    "rows = 5;\n",
    "for i in range(1, columns*rows +1):\n",
    "    img_xy = np.random.randint(len(trainset));\n",
    "    img = trainset[img_xy][0][0,:,:]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title(labels_map[trainset[img_xy][1]])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:20.899159Z",
     "start_time": "2020-07-29T16:23:20.877188Z"
    }
   },
   "outputs": [],
   "source": [
    "# check shape \n",
    "images, labels = next(iter(train_loader))\n",
    "print('data shape', images.shape)\n",
    "print('label shape', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:21.183131Z",
     "start_time": "2020-07-29T16:23:21.180433Z"
    }
   },
   "outputs": [],
   "source": [
    "print('data shape', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:21.618312Z",
     "start_time": "2020-07-29T16:23:21.614618Z"
    }
   },
   "outputs": [],
   "source": [
    "images.view(images.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Build\n",
    "\n",
    "``` \n",
    "model name = BaseNet\n",
    "Input, Hidden, output == (n_in=28*28*1, n_mid =80, n_out=10) \n",
    "1. Fully-connected-layer \n",
    "2. Activation(ReLU)\n",
    "3. Fully-connected-layer \n",
    "4. Activation(ReLu)\n",
    "5. Fully-connected-layer \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:23:22.530846Z",
     "start_time": "2020-07-29T16:23:22.528153Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T18:13:39.921136Z",
     "start_time": "2020-07-29T18:13:39.917425Z"
    }
   },
   "outputs": [],
   "source": [
    "model = \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:10:48.633776Z",
     "start_time": "2020-07-29T16:10:48.628427Z"
    }
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Num of Total Parameter : \",total_params)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Num of Trainable Parameter :\",trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss and Optimizer \n",
    "```\n",
    "Loss : CrossEntropyLoss\n",
    "Optimizer: Adam , lr= 1e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:10:48.831173Z",
     "start_time": "2020-07-29T16:10:48.826685Z"
    }
   },
   "outputs": [],
   "source": [
    "### Set Loss and Optimizer \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "loss_fn =\n",
    "optimizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test\n",
    "\n",
    "- model.train() == (Trian mode)\n",
    "- model.eval() == (Evalutaion mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:35:08.516504Z",
     "start_time": "2020-07-29T16:35:08.511743Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()# 학습일때는 학습모드로 설정\n",
    "\n",
    "    for data,targets in train_loader:\n",
    "        # 1)\n",
    "        # 2)\n",
    "        outputs = model(data) # 데이터를 입력하고 출력을 계산\n",
    "        loss = loss_fn(outputs,targets) # 출력과 학습 데이터의 정답 간의 오차를 계산\n",
    "        # 3 ) 오차를 역전파하여 계산함\n",
    "        # 4) 역전파 계산한 값으로 가중치를 수정\n",
    "\n",
    "    print(f'[TRAIN] Epoch {epoch} \\t Loss: {loss.item():1.5f}',end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:35:08.984071Z",
     "start_time": "2020-07-29T16:35:08.978458Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval() # 추론할때는 추론모드로! Dropout이나 Batch-Norm과 같은 기법에선 특히!\n",
    "    correct = 0\n",
    "    \n",
    "   # 1)\n",
    "        for data, targets in test_loader:\n",
    "            # 2) \n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data,1) # select maximum probability \n",
    "            correct += predicted.eq(targets.data.view_as(predicted)).sum() # if collect +1 \n",
    "\n",
    "    # Accuracy print\n",
    "    data_num = len(test_loader.dataset) \n",
    "    print(f'[TEST] Epoch {epoch} \\t Accuracy: {correct}/{data_num} ({100.*correct/data_num :3.5f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T16:11:47.428564Z",
     "start_time": "2020-07-29T16:10:49.500190Z"
    }
   },
   "outputs": [],
   "source": [
    "Epochs = 10\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    train(epoch+1)\n",
    "    test(epoch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise :  3 problems \n",
    "  </h1>\n",
    "</div>\n",
    "\n",
    "**Compare loss with below models: (10 epochs)**\n",
    "\n",
    "1. Linear - Activation - DropOut - BatchNorm - Linear - Activation - DropOut - BatchNorm - Linear -> Output\n",
    "\n",
    "2. Linear - Activation - BatchNorm - Dropout - Linear - Activation - BatchNorm - BatchNorm - Linear--> Output\n",
    "\n",
    "3. Linear - BatchNorm - Activation - Dropout - Linear - Activation - BatchNorm - Dropout - Linear --> Output"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "546.903px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
