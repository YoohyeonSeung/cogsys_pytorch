{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture_17_VGG16\n",
    "\n",
    "<font size=5><b><b></font>\n",
    "<div align='right'> Hoe Sung Ryu ( 류 회 성 ) </div>\n",
    "<div align='right'> Minsuk Sung ( 성 민 석) </div>\n",
    "    \n",
    "    \n",
    "\n",
    "<img src='../img/Vggnet.png' width=50%>\n",
    "     \n",
    "    \n",
    "    \n",
    "> Author: Hoe Sung Ryu, Minsuk Sung  <p>\n",
    "> Tel: 010-6636-7275 / skainf23@gamil.com // 010-5134-3621 / mssung94@gmail.com  <p>\n",
    "> 본 내용은 파이토치를 활용한 딥러닝 과외 자료입니다. 본 내용을 제작자의 동의없이 무단으로 복제하는 행위는 금합니다.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "Syllabus\n",
    "    \n",
    "|Event Type|Date|Topic|\n",
    "|--:|:---:|:---|\n",
    "|1 |July 27| Environment setting and Python basic|\n",
    "|2 |July 28| Pytorch basic and Custom Data load |\n",
    "|3 |July 29| Traditional Machine Learning(1) |\n",
    "|4 |July 30| Traditional Machine Learning(2) |\n",
    "|5 |July 31| CNN(Convolutional Neural Network)(1)  |\n",
    "|6 |Aug 03| CNN(Convolutional NeuralNetwork)(2) |\n",
    "|7 |Aug 04|  RNN(Recurrent Neural Networks)(1) |\n",
    "|8 |Aug 05|  RNN(Recurrent Neural Networks)(2) |\n",
    "|9 |Aug 06|  Transfer learning(VGG pertained on ImageNEt for CIfar-10)| \n",
    "|10|Aug 07|**Mini_Kaggle**: Facial Expression Recognition on `AffectNet` | \n",
    "|11|Aug 08|`Awards` and `Closing`| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Today\" data-toc-modified-id=\"Today-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Today</a></span></li><li><span><a href=\"#VGGNet\" data-toc-modified-id=\"VGGNet-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>VGGNet</a></span><ul class=\"toc-item\"><li><span><a href=\"#AlexNet-vs-VGGNet\" data-toc-modified-id=\"AlexNet-vs-VGGNet-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>AlexNet vs VGGNet</a></span></li></ul></li><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#VGG-16-models\" data-toc-modified-id=\"VGG-16-models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>VGG-16 models</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "\n",
    "- VGGNet\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet \n",
    "\n",
    "![](https://1.bp.blogspot.com/-pBSrav9x1bs/XXjIQXMH_mI/AAAAAAAAF0w/ZPczthmkt0U6A0EoW0tUH5d1BwrPdgvrwCLcBGAsYHQ/s1600/VGGNet%2BDuring%2BTesting.png)\n",
    "\n",
    "ex)\n",
    "- conv1-512: filter_size=1 //the number filters=512  \n",
    "- conv3-512: filter_size=3 //the number filters=512  \n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "- [1] Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet vs VGGNet\n",
    "![](https://kakalabblog.files.wordpress.com/2017/04/slide034-e1491546326293.jpg?w=1088)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:16:55.523152Z",
     "start_time": "2020-08-02T10:16:54.578783Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:56:38.189379Z",
     "start_time": "2020-08-02T15:56:36.908014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Files already downloaded and verified\n",
      "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "# num_features = 784 \n",
    "num_features = 32*32\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "## Datasets and DataLoader \n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise :  VGG-16\n",
    "  </h1>\n",
    "</div>\n",
    "\n",
    "## VGG-16 models\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "-> Conv3-64(kernel_size=3, stride=1,padding=1)\n",
    "-> nn.ReLU()\n",
    "-> Conv3-64\n",
    "-> nn.ReLU()\n",
    "-> Maxpooling(kernel_size=2, stride=2)\n",
    "\n",
    "-> conv3-128\n",
    "-> nn.ReLU()\n",
    "-> Conv3-128\n",
    "-> nn.ReLU()\n",
    "-> Maxpooling(kernel_size=2, stride=2)\n",
    "\n",
    "-> Conv3-256\n",
    "-> nn.ReLU()\n",
    "-> Conv3-256\n",
    "-> nn.ReLU()\n",
    "-> Conv3-256\n",
    "-> nn.ReLU()\n",
    "-> Maxpooling(kernel_size=2, stride=2)\n",
    "\n",
    "-> Conv3-512\n",
    "-> nn.ReLU()\n",
    "-> Conv3-512\n",
    "-> nn.ReLU()\n",
    "-> Conv3-512\n",
    "-> nn.ReLU()\n",
    "-> Maxpooling(kernel_size=2, stride=2)\n",
    "\n",
    "-> Conv3-512\n",
    "-> nn.ReLU()\n",
    "-> Conv3-512\n",
    "-> nn.ReLU()\n",
    "-> Conv3-512\n",
    "-> nn.ReLU()\n",
    "-> Maxpooling(kernel_size=2, stride=2)\n",
    "\n",
    "-> Fully connected layer- ???, 4096\n",
    "-> nn.ReLU(),\n",
    "-> Fully connected layer- 4096 ,???\n",
    "-> nn.ReLU()\n",
    "-> Fully connected layer- 4096,???\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T16:16:29.821184Z",
     "start_time": "2020-08-02T16:16:29.443758Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = VGG16(num_features=num_features,\n",
    "              num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T16:16:30.287512Z",
     "start_time": "2020-08-02T16:16:30.283199Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T16:27:21.698501Z",
     "start_time": "2020-08-02T16:27:21.689738Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "#         logits, probas = model(features)\n",
    "\n",
    "        predicted= model(features)\n",
    "        _, predicted_labels = torch.max(predicted, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "#             logits, probas = model(features)\n",
    "            predicted= model(features)\n",
    "#             loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            loss = criterion(predicted,targets,reduction='sum' )\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T16:27:22.313220Z",
     "start_time": "2020-08-02T16:27:22.310261Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T16:32:59.793821Z",
     "start_time": "2020-08-02T16:27:22.818848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 46.9211\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 46.9211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9e4be0f5f686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         cost.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "#         print(features.shape)\n",
    "#         break\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "#         logits, probas = model(features)\n",
    "#         cost = F.cross_entropy(logits, targets)\n",
    "\n",
    "        predicted = model(features)\n",
    "        loss = criterion(predicted,targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% |  Loss: %.3f' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader),\n",
    "              compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
