{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec_14_Convolutional Neural Network\n",
    "<font size=5><b><b></font>\n",
    "<div align='right'> Hoe Sung Ryu ( 류 회 성 ) </div>\n",
    "    \n",
    "<img src= https://adeshpande3.github.io/assets/Cover.png width=70%>\n",
    "\n",
    "    \n",
    "---\n",
    "    \n",
    "Syllabus\n",
    "\n",
    "|Event Type|Date|Topic|\n",
    "|--:|:---:|:---|\n",
    "|1 |July 27| Environment setting and Python basic|\n",
    "|2 |July 28| Pytorch basic and Custom Data load |\n",
    "|3 |July 29| Traditional Machine Learning(1) |\n",
    "|4 |July 30| Traditional Machine Learning(2) |\n",
    "|5 |July 31| CNN(Convolutional Neural Network)(1)  |\n",
    "|6 |Aug 03| CNN(Convolutional NeuralNetwork)(2) |\n",
    "|7 |Aug 04|  RNN(Recurrent Neural Networks)(1) |\n",
    "|8 |Aug 05|  RNN(Recurrent Neural Networks)(2) |\n",
    "|9 |Aug 06|  Transfer learning(VGG pertained on ImageNEt for CIfar-10)| \n",
    "|10|Aug 07|**Mini_Kaggle**: Facial Expression Recognition on `AffectNet` | \n",
    "|11|Aug 08|`Awards` and `Closing`| \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Today\" data-toc-modified-id=\"Today-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Today</a></span></li><li><span><a href=\"#Basic-CNN\" data-toc-modified-id=\"Basic-CNN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Basic CNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-Convolutional-Networks\" data-toc-modified-id=\"What-is-Convolutional-Networks-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>What is Convolutional Networks</a></span></li><li><span><a href=\"#Several-architectures-in-the-field-of-Convolutional-Networks\" data-toc-modified-id=\"Several-architectures-in-the-field-of-Convolutional-Networks-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Several architectures in the field of Convolutional Networks</a></span></li></ul></li><li><span><a href=\"#Conv2d\" data-toc-modified-id=\"Conv2d-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Conv2d</a></span><ul class=\"toc-item\"><li><span><a href=\"#Kernel\" data-toc-modified-id=\"Kernel-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Kernel</a></span></li><li><span><a href=\"#Stride-and-Padding\" data-toc-modified-id=\"Stride-and-Padding-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stride and Padding</a></span></li></ul></li><li><span><a href=\"#Pooling\" data-toc-modified-id=\"Pooling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pooling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Average-Pooling-Layer\" data-toc-modified-id=\"Average-Pooling-Layer-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Average Pooling Layer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Max-Pooling-Layer\" data-toc-modified-id=\"Max-Pooling-Layer-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Max Pooling Layer</a></span></li></ul></li><li><span><a href=\"#The-number-of-outputs\" data-toc-modified-id=\"The-number-of-outputs-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>The number of outputs</a></span></li><li><span><a href=\"#Settings\" data-toc-modified-id=\"Settings-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Settings</a></span></li></ul></li><li><span><a href=\"#Dataset-and-DataLoader\" data-toc-modified-id=\"Dataset-and-DataLoader-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dataset and DataLoader</a></span></li><li><span><a href=\"#Model-build\" data-toc-modified-id=\"Model-build-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model build</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Compile\" data-toc-modified-id=\"Model-Compile-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Model Compile</a></span></li><li><span><a href=\"#Loss-and-Optimizer\" data-toc-modified-id=\"Loss-and-Optimizer-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Loss and Optimizer</a></span></li></ul></li><li><span><a href=\"#Training-and-Evaluation\\\" data-toc-modified-id=\"Training-and-Evaluation\\-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training and Evaluation\\</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today \n",
    "\n",
    "1. Basic concept of CNN \n",
    "\n",
    "\n",
    "2. LeNet\n",
    "> trained on MNIST digit dataset with 60k examples Average pooling\n",
    "uses sigmoid or tanh non-linearity\n",
    "\n",
    "\n",
    "3. AlexNet\n",
    "> - 60million parameters\n",
    "> - Non-linearity : uses ReLU\n",
    "> - new methods : dropout, data augmentation\n",
    "> - 5 Convolutional Layers, 3 Fully-Connected layers, 1000-way softmax\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN\n",
    "\n",
    "Reference - [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "\n",
    "### What is Convolutional Networks\n",
    "Special type of feedforward NN, using spatial information of inputs with convolution operations\n",
    "Reduces the number of parameters designed for vision tasks(which usually has too many parameters when fully-connected). \n",
    "\n",
    "We will go into more details below, but a `simple ConvNet` for `Fahsion-MNIST `\n",
    "classification could have the architecture:\n",
    "\n",
    "> **Conv** Layer$\\rightarrow$ Activatetion $\\rightarrow$ **Max Pooling** Layer $\\rightarrow$ **Fully Connected** Layer $\\rightarrow$ **Softmax** Layer\n",
    "\n",
    "\n",
    "### Several architectures in the field of Convolutional Networks \n",
    "\n",
    "\n",
    "The most common are:\n",
    "\n",
    "> - LeNet. The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc.\n",
    "\n",
    "> - AlexNet. The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).\n",
    "\n",
    "> - ZF Net. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet (short for Zeiler & Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n",
    "\n",
    "> - GoogLeNet. The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently Inception-v4.\n",
    "\n",
    "> - VGGNet. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n",
    "\n",
    "> - ResNet. Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming’s presentation (video, slides), and some recent experiments that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016).In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d\n",
    "\n",
    "<img src =https://camo.githubusercontent.com/4f638bd0366d39092c4107f7afc9bbc297215898/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f323334302f312a46772d6568634e42523962794874686f2d52786274772e676966 width=40%>\n",
    "\n",
    "```python\n",
    "nn.Conv2d(\n",
    "    in_channels, # input channel \n",
    "    out_channels, # output channel\n",
    "    kernel_size, # size of kernel(weights) \n",
    "    stride=1, # stride\n",
    "    padding=0, # padding\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "    bias=True,\n",
    "    padding_mode='zeros',\n",
    ")\n",
    "```\n",
    "reference: https://pytorch.org/docs/stable/nn.html#conv2d\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel \n",
    "<img src=https://d2l.ai/_images/conv-pad.svg>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride and Padding \n",
    "\n",
    "<img src=https://camo.githubusercontent.com/e6b0378ae2dbc28d48599bf971fe7aacae77d35c/68747470733a2f2f692e737461636b2e696d6775722e636f6d2f707444505a2e676966 width=60%>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling \n",
    "\n",
    "\n",
    "Downsamples the image size : to use fewer parameters for next layers Max pooling, Average pooling widely used\n",
    "Subsampling pixels will not change the object\n",
    "\n",
    "- Average Pooling Layer\n",
    "- Max Pooling Layer \n",
    "\n",
    "\n",
    "\n",
    "### Average Pooling Layer\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png' width=50%>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "torch.nn.AvgPool2d(kernel_size,\n",
    "                   stride=None, \n",
    "                   padding=0, \n",
    "                   ceil_mode= False,\n",
    "                   count_include_pad=True,\n",
    "                   divisor_override= None)\n",
    "```\n",
    "refer.: [Pytorch official_ Averagepooling](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d)\n",
    "\n",
    "\n",
    "#### Max Pooling Layer\n",
    "\n",
    "- 일정 크기의 구간 내에서 가장 큰 값만을 전달하고 다른 정보는 버리는 방법\n",
    "- CNN에서는 일정 구간에서 해당 필터의 모양과 가장 비슷한 부분을 전달하는 연산\n",
    "\n",
    "<img src='../img/average_pooling.png' width=50%>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "torch.nn.functional.max_pool2d(kernel_size, # kernel \n",
    "                               stride=None, # stride\n",
    "                               padding=0, # padding\n",
    "                               dilation=1,\n",
    "                               return_indices=False, \n",
    "                               ceil_mode=False)\n",
    "```\n",
    "\n",
    "refer.: [Pytorch official_ Maxpooling](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of outputs\n",
    "\n",
    "- image_size: 28*28\n",
    "- \\# of filters: 5\n",
    "- padding =  1\n",
    "- stride = 2 \n",
    "\n",
    "```python\n",
    "def cnn_param(W,F,P, S=1):\n",
    "    \"\"\"\n",
    "    W = width or height\n",
    "    F = Filters\n",
    "    P = Padding \n",
    "    S = stride\n",
    "    \"\"\"\n",
    "    return (((W - F + 2*P) / S) + 1 )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T12:23:32.721548Z",
     "start_time": "2020-07-30T12:23:32.718444Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnn_param(W,F,P, S):\n",
    "    return (((W - F + 2*P) / S) + 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T12:23:36.081152Z",
     "start_time": "2020-07-30T12:23:36.077817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5\n"
     ]
    }
   ],
   "source": [
    "out1= cnn_param(W= 28, F=5, P=1, S=2)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Convolutional Neural Network with Fashion MNIST \n",
    "  </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T10:50:53.496458Z",
     "start_time": "2020-07-30T10:50:53.493803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparam\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epoch = 10\n",
    "\n",
    "# GPU\n",
    "device = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T10:52:00.745844Z",
     "start_time": "2020-07-30T10:52:00.721482Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/',\n",
    "                                 download=False,\n",
    "                                 train=True, \n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.5),\n",
    "                                                                                    (0.5))]))\n",
    "\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/',\n",
    "                                 download=False,\n",
    "                                 train=False, \n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.5),\n",
    "                                                                                    (0.5))]))\n",
    "\n",
    "\n",
    "# Set dataLoader\n",
    "##### TODO ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model build \n",
    "\n",
    "Fill the blank using `cnn_param` functions above:\n",
    "```python \n",
    "def cnn_param(W,F,P, S=1):\n",
    "    \"\"\"\n",
    "    W = width or height\n",
    "    F = Filters\n",
    "    P = Padding \n",
    "    S = stride\n",
    "    \"\"\"\n",
    "    return (((W - F + 2*P) / S) + 1 )\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "```python \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,5),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(??????????, ???????????),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,10)\n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO ##### \n",
    "class CNN(nn.Module):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T12:36:02.510301Z",
     "start_time": "2020-07-30T12:36:02.397539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-477cd6afb73e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CNN' is not defined"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T10:52:20.130895Z",
     "start_time": "2020-07-30T10:52:20.125930Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Total Parameter :  123222\n",
      "Num of Trainable Parameter : 123222\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Num of Total Parameter : \",total_params)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Num of Trainable Parameter :\",trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer\n",
    "\n",
    "```\n",
    "Loss: CrossEntropy \n",
    "optimizer: SGD lr = learning_rate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T10:52:54.211147Z",
     "start_time": "2020-07-30T10:52:54.207716Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn \n",
    "from torch import optim\n",
    "\n",
    "##### TODO ##### \n",
    "loss_fn = \n",
    "optimizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T10:56:49.909498Z",
     "start_time": "2020-07-30T10:52:56.110070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  0 Batch    0/3750] Loss: 2.3104565143585205\n",
      "[Epoch  0 Batch 1000/3750] Loss: 2.270261526107788\n",
      "[Epoch  0 Batch 2000/3750] Loss: 2.1503348350524902\n",
      "[Epoch  0 Batch 3000/3750] Loss: 1.0309312343597412\n",
      "-------------------------------------------------------------\n",
      "[Epoch  0] Accuracy of train dataset: 66.1816635131836\n",
      "[Epoch  0] Accuracy of test dataset: 65.47000122070312\n",
      "-------------------------------------------------------------\n",
      "[Epoch  1 Batch    0/3750] Loss: 0.8954789638519287\n",
      "[Epoch  1 Batch 1000/3750] Loss: 0.5684710741043091\n",
      "[Epoch  1 Batch 2000/3750] Loss: 1.0102077722549438\n",
      "[Epoch  1 Batch 3000/3750] Loss: 0.5225096344947815\n",
      "-------------------------------------------------------------\n",
      "[Epoch  1] Accuracy of train dataset: 72.66166687011719\n",
      "[Epoch  1] Accuracy of test dataset: 71.79000091552734\n",
      "-------------------------------------------------------------\n",
      "[Epoch  2 Batch    0/3750] Loss: 0.5232093334197998\n",
      "[Epoch  2 Batch 1000/3750] Loss: 0.9679268002510071\n",
      "[Epoch  2 Batch 2000/3750] Loss: 0.8773658871650696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-52b7c60c040e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9d4e53df797a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "    \n",
    "for i in range(num_epoch):\n",
    "    # Train CNN \n",
    "    model.train()\n",
    "    \n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        X_train = ##### TODO ##### \n",
    "        y_train = ##### TODO ##### \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(##### TODO ##### )\n",
    "        loss = loss_fn(##### TODO ##### , ##### TODO ##### )\n",
    "        loss.backward()\n",
    "        ##### TODO ##### \n",
    "        \n",
    "        if j % 1000 == 0:\n",
    "            print(f'[Epoch {i:2d} Batch {j:>4d}/{len(train_loader):>4d}] Loss: {loss}')\n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image,label in train_loader:\n",
    "            X_train = ##### TODO ##### \n",
    "            y_train = ##### TODO ##### \n",
    "\n",
    "            output = ##### TODO ##### \n",
    "            train_loss = loss_fn(output,##### TODO ##### )\n",
    "            _,output_index = torch.max(output,1)\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (output_index == y_train).sum().float()\n",
    "            \n",
    "    train_acc = 100*correct/total\n",
    "    print(f\"[Epoch {i:2d}] Accuracy of train dataset: {train_acc}\")\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Test CNN \n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image,label in test_loader:\n",
    "            X_test = ##### TODO ##### \n",
    "            y_test = ##### TODO ##### \n",
    "\n",
    "            output = ##### TODO ##### \n",
    "            test_loss = loss_fn(##### TODO ##### ,##### TODO ##### )\n",
    "            _,output_index = torch.max(output,1)\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (output_index == y_test).sum().float()\n",
    "            \n",
    "    test_acc = 100*correct/total\n",
    "    print(f\"[Epoch {i:2d}] Accuracy of test dataset: {test_acc}\")\n",
    "    print('-------------------------------------------------------------')\n",
    "    test_accs.append(test_acc)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
