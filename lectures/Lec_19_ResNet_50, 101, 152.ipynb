{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec_19_ResNet\n",
    "\n",
    "<font size=5><b><b></font>\n",
    "<div align='right'> Hoe Sung Ryu ( 류 회 성 ) </div>\n",
    "<div align='right'> Minsuk Sung ( 성 민 석) </div>\n",
    "    \n",
    "    \n",
    "\n",
    "![](https://miro.medium.com/max/6652/1*OFfO8VzLv8GNFNRKafvB7w.png)\n",
    "     \n",
    "    \n",
    "    \n",
    "> Author: Hoe Sung Ryu, Minsuk Sung  <p>\n",
    "> Tel: 010-6636-7275 / skainf23@gamil.com // 010-5134-3621 / mssung94@gmail.com  <p>\n",
    "> 본 내용은 파이토치를 활용한 딥러닝 과외 자료입니다. 본 내용을 제작자의 동의없이 무단으로 복제하는 행위는 금합니다.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "Syllabus\n",
    "    \n",
    "|Event Type|Date|Topic|\n",
    "|--:|:---:|:---|\n",
    "|1 |July 27| Environment setting and Python basic|\n",
    "|2 |July 28| Pytorch basic and Custom Data load |\n",
    "|3 |July 29| Traditional Machine Learning(1) |\n",
    "|4 |July 30| Traditional Machine Learning(2) |\n",
    "|5 |July 31| CNN(Convolutional Neural Network)(1)  |\n",
    "|6 |Aug 03| CNN(Convolutional NeuralNetwork)(2) |\n",
    "|7 |Aug 04|  RNN(Recurrent Neural Networks)(1) |\n",
    "|8 |Aug 05|  RNN(Recurrent Neural Networks)(2) |\n",
    "|9 |Aug 06|  Transfer learning(VGG pertained on ImageNEt for CIfar-10)| \n",
    "|10|Aug 07|**Mini_Kaggle**: Facial Expression Recognition on `AffectNet` | \n",
    "|11|Aug 08|`Awards` and `Closing`| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ResNet\" data-toc-modified-id=\"ResNet-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ResNet</a></span><ul class=\"toc-item\"><li><span><a href=\"#residual-block\" data-toc-modified-id=\"residual-block-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>residual block</a></span></li></ul></li><li><span><a href=\"#Pretrained-models-in--PyTorch\" data-toc-modified-id=\"Pretrained-models-in--PyTorch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pretrained models in  PyTorch</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "![](https://miro.medium.com/max/6652/1*OFfO8VzLv8GNFNRKafvB7w.png)\n",
    "\n",
    "\n",
    "References\n",
    "- [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). ([CVPR Link](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html))\n",
    "\n",
    "- [2] Zhang, K., Tan, L., Li, Z., & Qiao, Y. (2016). Gender and smile classification using deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 34-38)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### residual block\n",
    "The ResNet-50 architecture is similar to the ResNet-34 architecture shown below (from [1]):\n",
    "<img src='../img/resnet-50-bottleneck.png'>\n",
    "\n",
    "However, in ResNet-50, the skip connection uses a bottleneck (from [1]):\n",
    "\n",
    "![](../img/resnet50-arch-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pretrained models in  PyTorch\n",
    "\n",
    "Pretrained models\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet\n",
    "- Inception v3\n",
    "- GoogLeNet\n",
    "- ShuffleNet v2\n",
    "- MobileNet v2\n",
    "- ResNeXt\n",
    "- Wide ResNet\n",
    "- MNASNet\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()\n",
    "vgg16 = models.vgg16()\n",
    "squeezenet = models.squeezenet1_0()\n",
    "densenet = models.densenet161()\n",
    "inception = models.inception_v3()\n",
    "googlenet = models.googlenet()\n",
    "shufflenet = models.shufflenet_v2_x1_0()\n",
    "mobilenet = models.mobilenet_v2()\n",
    "resnext50_32x4d = models.resnext50_32x4d()\n",
    "wide_resnet50_2 = models.wide_resnet50_2()\n",
    "mnasnet = models.mnasnet1_0()\n",
    "```\n",
    "\n",
    "reference: [PyTorch TORCHVISON.MODELS](https://pytorch.org/docs/stable/torchvision/models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise :  ResNet_50\n",
    "  </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:30:41.105848Z",
     "start_time": "2020-08-02T17:30:41.082543Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        \"\"\"\n",
    "        identity_downsample = dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(in_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) # bn\n",
    "        self.conv1 = nn.Conv2d(out_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=3,\n",
    "                               stride=stride,\n",
    "                               padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1 = nn.Conv2d(out_channels,\n",
    "                               out_channels*self.expansion,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "        def forward(self, x):\n",
    "            identity = x\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "\n",
    "            # Residual reshape !!!\n",
    "            if self.identity_downsample is not None:\n",
    "                identity = self.identity_downsample(identity)\n",
    "            x += identity\n",
    "\n",
    "            x = self.relu(x)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # layer1_block\n",
    "        self.layer1 = self._maker_layer(block, layers[0], out_channels=64, stride=1)\n",
    "        # layer2_block\n",
    "        self.layer2 = self._maker_layer(block, layers[1], out_channels=128, stride=1)\n",
    "        # layer3_block\n",
    "        self.layer3 = self._maker_layer(block, layers[2], out_channels=256, stride=1)\n",
    "        # layer4_block\n",
    "        self.layer4 = self._maker_layer(block, layers[3], out_channels=512, stride=1)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*4, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(X)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # layer \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "        \n",
    "\n",
    "    def _maker_layer(self, block, num_residual_blocks, out_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if (stride != 1) or (self.in_channels != out_channels * 4):\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, \n",
    "                                                          out_channels*4,\n",
    "                                                          kernel_size=1,\n",
    "                                                          stride=stride),\n",
    "                                                nn.BatchNorm2d(out_channels*4)\n",
    "                                               )\n",
    "        # 1st residual block \n",
    "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
    "        \n",
    "        # add rest residual block \n",
    "        for i in range(num_residual_blocks -1):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "            \n",
    "        return nn.Sequential(*layers)                        \n",
    "    \n",
    "    \n",
    "    \n",
    "def ResNet50(img_channels, num_classess):\n",
    "    return ResNet(block,[3,4,6,3], img_channels, num_classess)\n",
    "\n",
    "def ResNet101(img_channels, num_classess):\n",
    "    return ResNet(block,[3,4,23,3], img_channels, num_classess)\n",
    "\n",
    "def ResNet152(img_channels, num_classess):\n",
    "    return ResNet(block,[3,8,36,3], img_channels, num_classess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:30:41.894138Z",
     "start_time": "2020-08-02T17:30:41.771169Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet50(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:31:49.568401Z",
     "start_time": "2020-08-02T17:31:49.559655Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LEARNING_RATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b76ca3a9633c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "import torch.nn as nn \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "#         logits, probas = model(features)\n",
    "\n",
    "        predicted= model(features)\n",
    "        _, predicted_labels = torch.max(predicted, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "#             logits, probas = model(features)\n",
    "            predicted= model(features)\n",
    "#             loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            loss = criterion(predicted,targets,reduction='sum' )\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "#         print(features.shape)\n",
    "#         break\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "#         logits, probas = model(features)\n",
    "#         cost = F.cross_entropy(logits, targets)\n",
    "\n",
    "        predicted = model(features)\n",
    "        loss = criterion(predicted,targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% |  Loss: %.3f' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader),\n",
    "              compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
